{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %load Walmart-NN-6.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import xgboost\n",
    "\n",
    "import theano\n",
    "from lasagne import layers, nonlinearities\n",
    "from nolearn.lasagne import NeuralNet\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('./train.csv') #Last visit number is 191347\n",
    "test = pd.read_csv('./test.csv') #Last visit number is 191348\n",
    "\n",
    "full_df = pd.concat((train, test))\n",
    "\n",
    "full_df_negatives = full_df[full_df.ScanCount < 0]\n",
    "full_df_negatives_agg = full_df_negatives.groupby(['VisitNumber']).agg({'ScanCount':np.sum}) #Negative Feature Count\n",
    "\n",
    "full_df_uncategorized = full_df[pd.isnull(full_df.Upc)]\n",
    "full_df_uncategorized_agg = full_df_uncategorized.groupby(['VisitNumber']).agg({'ScanCount':np.sum}) #Unknown Feature Count\n",
    "\n",
    "full_df_totals = full_df[full_df.ScanCount > 0]\n",
    "full_df_totals_agg = full_df_totals.groupby(['VisitNumber']).agg({'ScanCount':np.sum}) #Total purchases Feature Count\n",
    "\n",
    "\n",
    "full_df.Upc.fillna(-100, inplace=True)\n",
    "full_df.DepartmentDescription.fillna('Unknown', inplace=True)\n",
    "full_df.FinelineNumber.fillna(-100, inplace=True)\n",
    "\n",
    "\n",
    "visit_days = full_df.loc[:,['VisitNumber','Weekday']]\n",
    "visit_days.drop_duplicates('VisitNumber', inplace = True)\n",
    "visit_days.set_index('VisitNumber', inplace = True)\n",
    "visit_days = pd.get_dummies(visit_days)\n",
    "\n",
    "full_df['FinelineNumber'] = full_df['FinelineNumber'].astype('int')\n",
    "full_df['DeptItems'] = full_df.DepartmentDescription +' ' + full_df.FinelineNumber.astype('str')\n",
    "\n",
    "full_deptitems_df = pd.pivot_table(full_df[full_df.ScanCount>0], values='ScanCount', index='VisitNumber',columns='DeptItems', aggfunc=np.sum)\n",
    "full_deptitems_df.fillna(0, inplace=True)\n",
    "\n",
    "\n",
    "y_df = full_df.loc[:, ['VisitNumber', 'TripType']]\n",
    "y_df.drop_duplicates('VisitNumber', inplace=True)\n",
    "y_df.set_index('VisitNumber', inplace=True)\n",
    "\n",
    "y_df = y_df.join(full_deptitems_df) #This requires an insane amount of memory **Cannot fill 0s due to memory error\n",
    "\n",
    "del full_deptitems_df\n",
    "\n",
    "X_train = y_df[pd.notnull(y_df.TripType)].drop('TripType', axis = 1).values\n",
    "X_test = y_df[pd.isnull(y_df.TripType)].drop('TripType', axis = 1).values\n",
    "y_train = y_df[pd.notnull(y_df.TripType)]['TripType'].values\n",
    "\n",
    "\n",
    "y_df = y_df[['TripType']] #Removing Unneccessary Columns\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train) #Splitting this into 2 cells works\n",
    "\n",
    "chi_sq_best = SelectKBest(score_func=chi2, k = 7000)\n",
    "chi_sq_best.fit(X_train,y_train)\n",
    "\n",
    "X_train = chi_sq_best.transform(X_train)\n",
    "\n",
    "X_test = np.nan_to_num(X_test)\n",
    "X_test = chi_sq_best.transform(X_test)\n",
    "\n",
    "X_df = pd.pivot_table(full_df, values='ScanCount', index='VisitNumber',columns='DepartmentDescription', aggfunc=np.sum)\n",
    "X_df.fillna(0, inplace=True)\n",
    "\n",
    "\n",
    "X_df = X_df.join(full_df_totals_agg, rsuffix='Totals')\n",
    "X_df = X_df.join(full_df_uncategorized_agg, rsuffix='Uncategorized')\n",
    "X_df = X_df.join(full_df_negatives_agg, rsuffix='Negatives')\n",
    "X_df = X_df.join(visit_days)\n",
    "X_df.fillna(0, inplace = True)\n",
    "\n",
    "y_df = y_df.join(X_df)\n",
    "\n",
    "X_train2 = y_df[pd.notnull(y_df.TripType)].drop('TripType', axis = 1).values\n",
    "X_test2 = y_df[pd.isnull(y_df.TripType)].drop('TripType', axis = 1).values\n",
    "y_train2 = y_df[pd.notnull(y_df.TripType)]['TripType'].values\n",
    "\n",
    "X_train = np.concatenate((X_train, X_train2), axis = 1)\n",
    "X_test = np.concatenate((X_test, X_test2), axis = 1)\n",
    "\n",
    "enc = LabelEncoder()\n",
    "y_train = enc.fit_transform(y_train)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 5000, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90674, 7079)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(95674, 7079)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "y_train = y_train.astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nn = NeuralNet(layers = [\n",
    "     ('input', layers.InputLayer),\n",
    "     ('dropout', layers.DropoutLayer),\n",
    "     ('hidden1', layers.DenseLayer),\n",
    "     ('dropout1', layers.DropoutLayer),   \n",
    "     ('hidden2', layers.DenseLayer),\n",
    "     ('dropout2', layers.DropoutLayer),   \n",
    "     ('output', layers.DenseLayer),],\n",
    "               \n",
    "     input_shape = (None, X_train.shape[1]),\n",
    "     dropout_p =.2,\n",
    "               \n",
    "     hidden1_num_units = 128,\n",
    "     dropout1_p = .2,\n",
    "     hidden2_num_units = 64,\n",
    "     dropout2_p = .2,\n",
    "               \n",
    "     output_num_units = np.unique(y_train).shape[0],\n",
    "     output_nonlinearity = nonlinearities.softmax,\n",
    "     \n",
    "     update_learning_rate = .0005,\n",
    "     update_momentum = .9,\n",
    "     \n",
    "     regression = False,\n",
    "     max_epochs = 300,\n",
    "     verbose = True\n",
    "      )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Neural Network with 916966 learnable parameters\n",
      "\n",
      "## Layer information\n",
      "\n",
      "  #  name        size\n",
      "---  --------  ------\n",
      "  0  input       7079\n",
      "  1  dropout     7079\n",
      "  2  hidden1      128\n",
      "  3  dropout1     128\n",
      "  4  hidden2       64\n",
      "  5  dropout2      64\n",
      "  6  output        38\n",
      "\n",
      "  epoch    train loss    valid loss    train/val    valid acc  dur\n",
      "-------  ------------  ------------  -----------  -----------  -----\n",
      "      1       \u001b[36m3.39475\u001b[0m       \u001b[32m3.14726\u001b[0m      1.07864      0.18060  3.04s\n",
      "      2       \u001b[36m3.06962\u001b[0m       \u001b[32m2.83186\u001b[0m      1.08396      0.30054  3.17s\n",
      "      3       \u001b[36m2.78868\u001b[0m       \u001b[32m2.53204\u001b[0m      1.10136      0.34979  3.00s\n",
      "      4       \u001b[36m2.54949\u001b[0m       \u001b[32m2.30432\u001b[0m      1.10639      0.38404  2.98s\n",
      "      5       \u001b[36m2.38179\u001b[0m       \u001b[32m2.13460\u001b[0m      1.11580      0.45392  2.93s\n",
      "      6       \u001b[36m2.25697\u001b[0m       \u001b[32m2.01102\u001b[0m      1.12230      0.49720  2.98s\n",
      "      7       \u001b[36m2.17200\u001b[0m       \u001b[32m1.91227\u001b[0m      1.13582      0.54197  3.04s\n",
      "      8       \u001b[36m2.09718\u001b[0m       \u001b[32m1.83584\u001b[0m      1.14236      0.55811  3.07s\n",
      "      9       \u001b[36m2.03741\u001b[0m       \u001b[32m1.76616\u001b[0m      1.15358      0.56869  2.93s\n",
      "     10       \u001b[36m1.98682\u001b[0m       \u001b[32m1.70834\u001b[0m      1.16302      0.57635  2.94s\n",
      "     11       \u001b[36m1.94019\u001b[0m       \u001b[32m1.65358\u001b[0m      1.17332      0.58803  2.96s\n",
      "     12       \u001b[36m1.90041\u001b[0m       \u001b[32m1.61238\u001b[0m      1.17863      0.59241  2.97s\n",
      "     13       \u001b[36m1.86770\u001b[0m       \u001b[32m1.57142\u001b[0m      1.18854      0.59956  2.92s\n",
      "     14       \u001b[36m1.82896\u001b[0m       \u001b[32m1.53551\u001b[0m      1.19111      0.60454  2.91s\n",
      "     15       \u001b[36m1.81297\u001b[0m       \u001b[32m1.50724\u001b[0m      1.20284      0.60900  2.92s\n",
      "     16       \u001b[36m1.77712\u001b[0m       \u001b[32m1.47722\u001b[0m      1.20302      0.61290  2.94s\n",
      "     17       \u001b[36m1.75200\u001b[0m       \u001b[32m1.45159\u001b[0m      1.20695      0.61967  3.04s\n",
      "     18       \u001b[36m1.73171\u001b[0m       \u001b[32m1.42616\u001b[0m      1.21425      0.62380  3.00s\n",
      "     19       \u001b[36m1.71010\u001b[0m       \u001b[32m1.40173\u001b[0m      1.21999      0.62724  2.94s\n",
      "     20       \u001b[36m1.69015\u001b[0m       \u001b[32m1.37968\u001b[0m      1.22503      0.63229  2.95s\n",
      "     21       \u001b[36m1.67681\u001b[0m       \u001b[32m1.36189\u001b[0m      1.23124      0.63408  2.96s\n",
      "     22       \u001b[36m1.65701\u001b[0m       \u001b[32m1.33954\u001b[0m      1.23700      0.63675  2.95s\n",
      "     23       \u001b[36m1.63873\u001b[0m       \u001b[32m1.32424\u001b[0m      1.23749      0.63971  3.01s\n",
      "     24       \u001b[36m1.62167\u001b[0m       \u001b[32m1.30556\u001b[0m      1.24213      0.64474  2.95s\n",
      "     25       \u001b[36m1.61247\u001b[0m       \u001b[32m1.29212\u001b[0m      1.24793      0.64293  2.86s\n",
      "     26       \u001b[36m1.60072\u001b[0m       \u001b[32m1.27932\u001b[0m      1.25122      0.64334  2.89s\n",
      "     27       \u001b[36m1.58078\u001b[0m       \u001b[32m1.26782\u001b[0m      1.24685      0.64593  2.98s\n",
      "     28       \u001b[36m1.57254\u001b[0m       \u001b[32m1.25276\u001b[0m      1.25526      0.65017  2.98s\n",
      "     29       \u001b[36m1.56311\u001b[0m       \u001b[32m1.24253\u001b[0m      1.25801      0.64881  2.91s\n",
      "     30       \u001b[36m1.54849\u001b[0m       \u001b[32m1.22935\u001b[0m      1.25960      0.65440  2.91s\n",
      "     31       \u001b[36m1.54341\u001b[0m       \u001b[32m1.21800\u001b[0m      1.26717      0.65640  2.96s\n",
      "     32       \u001b[36m1.52913\u001b[0m       \u001b[32m1.20595\u001b[0m      1.26799      0.65979  3.00s\n",
      "     33       \u001b[36m1.52431\u001b[0m       \u001b[32m1.19585\u001b[0m      1.27466      0.66322  2.97s\n",
      "     34       \u001b[36m1.50942\u001b[0m       \u001b[32m1.18577\u001b[0m      1.27295      0.66617  3.03s\n",
      "     35       \u001b[36m1.50587\u001b[0m       \u001b[32m1.17770\u001b[0m      1.27865      0.66800  2.98s\n",
      "     36       \u001b[36m1.48825\u001b[0m       \u001b[32m1.16652\u001b[0m      1.27580      0.67160  2.94s\n",
      "     37       \u001b[36m1.48430\u001b[0m       \u001b[32m1.16280\u001b[0m      1.27648      0.66816  3.00s\n",
      "     38       \u001b[36m1.47671\u001b[0m       \u001b[32m1.15071\u001b[0m      1.28330      0.67687  2.99s\n",
      "     39       \u001b[36m1.47548\u001b[0m       \u001b[32m1.14304\u001b[0m      1.29084      0.67671  2.94s\n",
      "     40       \u001b[36m1.45340\u001b[0m       \u001b[32m1.13917\u001b[0m      1.27584      0.67507  2.96s\n",
      "     41       1.45620       \u001b[32m1.13231\u001b[0m      1.28604      0.67723  2.92s\n",
      "     42       \u001b[36m1.45242\u001b[0m       \u001b[32m1.12968\u001b[0m      1.28569      0.67609  2.98s\n",
      "     43       \u001b[36m1.44650\u001b[0m       \u001b[32m1.11874\u001b[0m      1.29297      0.67861  3.04s\n",
      "     44       \u001b[36m1.43577\u001b[0m       \u001b[32m1.11334\u001b[0m      1.28960      0.67797  2.98s\n",
      "     45       1.43888       \u001b[32m1.10624\u001b[0m      1.30070      0.67927  2.93s\n",
      "     46       \u001b[36m1.42134\u001b[0m       \u001b[32m1.10555\u001b[0m      1.28564      0.67633  2.93s\n",
      "     47       \u001b[36m1.41442\u001b[0m       \u001b[32m1.09829\u001b[0m      1.28784      0.68126  2.95s\n",
      "     48       \u001b[36m1.40882\u001b[0m       \u001b[32m1.08972\u001b[0m      1.29283      0.68192  3.05s\n",
      "     49       \u001b[36m1.40723\u001b[0m       \u001b[32m1.08518\u001b[0m      1.29676      0.68342  2.99s\n",
      "     50       \u001b[36m1.40491\u001b[0m       \u001b[32m1.08464\u001b[0m      1.29527      0.68148  3.11s\n",
      "     51       \u001b[36m1.39582\u001b[0m       \u001b[32m1.07735\u001b[0m      1.29560      0.68397  3.04s\n",
      "     52       \u001b[36m1.39377\u001b[0m       \u001b[32m1.06953\u001b[0m      1.30317      0.68691  3.07s\n",
      "     53       \u001b[36m1.38954\u001b[0m       \u001b[32m1.06747\u001b[0m      1.30171      0.68461  2.87s\n",
      "     54       \u001b[36m1.38065\u001b[0m       \u001b[32m1.06670\u001b[0m      1.29432      0.68340  3.03s\n",
      "     55       \u001b[36m1.37131\u001b[0m       \u001b[32m1.05514\u001b[0m      1.29965      0.68691  2.95s\n",
      "     56       1.37344       1.05758      1.29866      0.68610  3.06s\n",
      "     57       \u001b[36m1.36654\u001b[0m       \u001b[32m1.05295\u001b[0m      1.29782      0.68593  3.00s\n",
      "     58       \u001b[36m1.36023\u001b[0m       \u001b[32m1.04822\u001b[0m      1.29766      0.68654  2.92s\n",
      "     59       \u001b[36m1.35771\u001b[0m       1.05035      1.29263      0.68764  2.99s\n",
      "     60       \u001b[36m1.35600\u001b[0m       \u001b[32m1.03931\u001b[0m      1.30472      0.68845  2.99s\n",
      "     61       \u001b[36m1.35207\u001b[0m       1.04238      1.29710      0.68296  2.94s\n",
      "     62       \u001b[36m1.34286\u001b[0m       \u001b[32m1.03014\u001b[0m      1.30357      0.69132  2.96s\n",
      "     63       1.34323       1.03522      1.29753      0.68582  3.03s\n",
      "     64       \u001b[36m1.33948\u001b[0m       \u001b[32m1.02620\u001b[0m      1.30528      0.68802  3.09s\n",
      "     65       \u001b[36m1.33786\u001b[0m       1.02788      1.30156      0.68676  2.99s\n",
      "     66       \u001b[36m1.32623\u001b[0m       \u001b[32m1.01908\u001b[0m      1.30140      0.68989  3.02s\n",
      "     67       1.32732       1.02212      1.29860      0.68428  3.14s\n",
      "     68       \u001b[36m1.32553\u001b[0m       \u001b[32m1.01706\u001b[0m      1.30330      0.68896  3.01s\n",
      "     69       \u001b[36m1.31984\u001b[0m       \u001b[32m1.01113\u001b[0m      1.30531      0.69149  2.99s\n",
      "     70       \u001b[36m1.31635\u001b[0m       \u001b[32m1.00911\u001b[0m      1.30447      0.69183  3.00s\n",
      "     71       1.31730       1.02116      1.29001      0.68523  2.95s\n",
      "     72       \u001b[36m1.31354\u001b[0m       \u001b[32m1.00867\u001b[0m      1.30225      0.68897  2.95s\n",
      "     73       \u001b[36m1.30654\u001b[0m       \u001b[32m1.00373\u001b[0m      1.30169      0.69088  3.05s\n",
      "     74       1.31084       \u001b[32m0.99649\u001b[0m      1.31546      0.69502  2.99s\n",
      "     75       \u001b[36m1.30127\u001b[0m       0.99754      1.30448      0.69178  3.00s\n",
      "     76       \u001b[36m1.29570\u001b[0m       \u001b[32m0.99608\u001b[0m      1.30080      0.69024  2.99s\n",
      "     77       \u001b[36m1.28929\u001b[0m       \u001b[32m0.99055\u001b[0m      1.30159      0.69436  2.94s\n",
      "     78       1.29065       0.99893      1.29204      0.69090  3.06s\n",
      "     79       1.28964       0.99469      1.29653      0.69075  3.02s\n",
      "     80       \u001b[36m1.28276\u001b[0m       \u001b[32m0.98554\u001b[0m      1.30159      0.69664  2.94s\n",
      "     81       1.28924       \u001b[32m0.98170\u001b[0m      1.31327      0.69742  2.98s\n",
      "     82       \u001b[36m1.28258\u001b[0m       \u001b[32m0.97812\u001b[0m      1.31128      0.69840  2.97s\n",
      "     83       \u001b[36m1.27669\u001b[0m       \u001b[32m0.97553\u001b[0m      1.30871      0.69631  2.95s\n",
      "     84       \u001b[36m1.27233\u001b[0m       \u001b[32m0.97461\u001b[0m      1.30547      0.69713  3.03s\n",
      "     85       \u001b[36m1.26931\u001b[0m       \u001b[32m0.97314\u001b[0m      1.30435      0.69619  3.10s\n",
      "     86       \u001b[36m1.26406\u001b[0m       0.97548      1.29584      0.69486  3.01s\n",
      "     87       1.26510       0.97346      1.29959      0.69653  3.13s\n",
      "     88       1.26546       \u001b[32m0.96919\u001b[0m      1.30569      0.69526  2.98s\n",
      "     89       \u001b[36m1.25235\u001b[0m       0.97007      1.29099      0.69515  2.94s\n",
      "     90       1.25876       \u001b[32m0.96443\u001b[0m      1.30518      0.69880  2.97s\n",
      "     91       1.25808       \u001b[32m0.96386\u001b[0m      1.30525      0.69653  2.95s\n",
      "     92       1.25452       \u001b[32m0.96083\u001b[0m      1.30566      0.69884  2.97s\n",
      "     93       \u001b[36m1.25126\u001b[0m       \u001b[32m0.95594\u001b[0m      1.30894      0.69929  2.93s\n",
      "     94       \u001b[36m1.24352\u001b[0m       \u001b[32m0.95558\u001b[0m      1.30131      0.70109  3.00s\n",
      "     95       \u001b[36m1.23830\u001b[0m       0.95768      1.29303      0.69808  3.00s\n",
      "     96       \u001b[36m1.23520\u001b[0m       \u001b[32m0.94963\u001b[0m      1.30071      0.70197  3.02s\n",
      "     97       1.24046       0.95092      1.30448      0.70065  3.13s\n",
      "     98       \u001b[36m1.23054\u001b[0m       0.95724      1.28550      0.69693  3.02s\n",
      "     99       1.23431       0.95167      1.29700      0.69900  3.05s\n",
      "    100       \u001b[36m1.22947\u001b[0m       0.95048      1.29353      0.70001  2.97s\n",
      "    101       1.23576       \u001b[32m0.94549\u001b[0m      1.30701      0.70208  2.95s\n",
      "    102       \u001b[36m1.22529\u001b[0m       0.94665      1.29435      0.70105  3.01s\n",
      "    103       \u001b[36m1.22274\u001b[0m       \u001b[32m0.94183\u001b[0m      1.29826      0.70160  3.24s\n",
      "    104       1.22350       \u001b[32m0.93935\u001b[0m      1.30249      0.70182  3.09s\n",
      "    105       \u001b[36m1.22000\u001b[0m       0.94389      1.29253      0.69808  3.17s\n",
      "    106       \u001b[36m1.21848\u001b[0m       0.94018      1.29601      0.70034  3.05s\n",
      "    107       \u001b[36m1.21414\u001b[0m       \u001b[32m0.93837\u001b[0m      1.29389      0.70243  3.12s\n",
      "    108       1.21539       0.93851      1.29501      0.69858  3.06s\n",
      "    109       \u001b[36m1.21402\u001b[0m       \u001b[32m0.93419\u001b[0m      1.29954      0.70248  3.03s\n",
      "    110       \u001b[36m1.21336\u001b[0m       0.93687      1.29512      0.70028  3.07s\n",
      "    111       \u001b[36m1.20612\u001b[0m       \u001b[32m0.93320\u001b[0m      1.29247      0.70078  3.11s\n",
      "    112       \u001b[36m1.20521\u001b[0m       \u001b[32m0.92661\u001b[0m      1.30067      0.70655  2.99s\n",
      "    113       \u001b[36m1.20326\u001b[0m       \u001b[32m0.92130\u001b[0m      1.30604      0.70875  2.98s\n",
      "    114       \u001b[36m1.19954\u001b[0m       0.92433      1.29774      0.70479  3.00s\n",
      "    115       \u001b[36m1.19680\u001b[0m       0.92192      1.29817      0.70567  3.00s\n",
      "    116       \u001b[36m1.19357\u001b[0m       0.92265      1.29363      0.70534  3.06s\n",
      "    117       \u001b[36m1.19185\u001b[0m       \u001b[32m0.91961\u001b[0m      1.29604      0.70644  3.03s\n",
      "    118       1.19544       0.92086      1.29818      0.70725  3.05s\n",
      "    119       1.19905       \u001b[32m0.91513\u001b[0m      1.31026      0.70985  3.08s\n",
      "    120       \u001b[36m1.19069\u001b[0m       0.91668      1.29891      0.70695  3.01s\n",
      "    121       \u001b[36m1.18297\u001b[0m       0.91526      1.29250      0.70683  2.99s\n",
      "    122       \u001b[36m1.17983\u001b[0m       \u001b[32m0.91389\u001b[0m      1.29101      0.70747  2.98s\n",
      "    123       \u001b[36m1.17551\u001b[0m       0.91424      1.28579      0.70699  3.05s\n",
      "    124       \u001b[36m1.17508\u001b[0m       \u001b[32m0.90948\u001b[0m      1.29204      0.70919  3.01s\n",
      "    125       1.17675       \u001b[32m0.90440\u001b[0m      1.30114      0.71134  2.99s\n",
      "    126       \u001b[36m1.16883\u001b[0m       0.90706      1.28859      0.70809  3.03s\n",
      "    127       1.17592       0.91147      1.29013      0.70661  2.99s\n",
      "    128       1.16891       0.91232      1.28124      0.70625  3.07s\n",
      "    129       \u001b[36m1.16877\u001b[0m       0.90468      1.29191      0.70987  3.00s\n",
      "    130       \u001b[36m1.16858\u001b[0m       0.90619      1.28955      0.70739  2.99s\n",
      "    131       \u001b[36m1.16016\u001b[0m       0.90600      1.28053      0.70844  2.99s\n",
      "    132       1.16102       \u001b[32m0.90247\u001b[0m      1.28648      0.70930  2.98s\n",
      "    133       \u001b[36m1.15704\u001b[0m       0.90406      1.27983      0.70754  3.00s\n",
      "    134       1.16136       \u001b[32m0.89253\u001b[0m      1.30120      0.71257  3.03s\n",
      "    135       \u001b[36m1.15257\u001b[0m       0.89577      1.28667      0.71235  3.10s\n",
      "    136       \u001b[36m1.15153\u001b[0m       0.89609      1.28507      0.71196  3.02s\n",
      "    137       1.15429       0.89881      1.28425      0.71011  2.98s\n",
      "    138       \u001b[36m1.14978\u001b[0m       0.89303      1.28751      0.71203  3.02s\n",
      "    139       \u001b[36m1.14453\u001b[0m       0.89333      1.28119      0.71187  3.02s\n",
      "    140       1.14886       \u001b[32m0.89069\u001b[0m      1.28985      0.71319  2.94s\n",
      "    141       \u001b[36m1.14404\u001b[0m       0.89627      1.27645      0.71060  3.03s\n",
      "    142       \u001b[36m1.14260\u001b[0m       \u001b[32m0.88771\u001b[0m      1.28713      0.71440  3.03s\n",
      "    143       1.14283       0.89075      1.28300      0.71209  3.04s\n",
      "    144       \u001b[36m1.13954\u001b[0m       0.89184      1.27773      0.71236  3.05s\n",
      "    145       1.14289       \u001b[32m0.88283\u001b[0m      1.29458      0.71792  2.95s\n",
      "    146       \u001b[36m1.12712\u001b[0m       0.88298      1.27649      0.71440  3.03s\n",
      "    147       1.13372       \u001b[32m0.87900\u001b[0m      1.28979      0.71748  3.02s\n",
      "    148       \u001b[36m1.12709\u001b[0m       0.87963      1.28133      0.71605  3.00s\n",
      "    149       1.13283       0.88199      1.28439      0.71474  2.99s\n",
      "    150       \u001b[36m1.12465\u001b[0m       0.88459      1.27139      0.71253  3.01s\n",
      "    151       \u001b[36m1.12065\u001b[0m       \u001b[32m0.87842\u001b[0m      1.27575      0.71826  3.07s\n",
      "    152       1.12489       \u001b[32m0.87372\u001b[0m      1.28747      0.71902  2.99s\n",
      "    153       1.12320       0.88256      1.27266      0.71443  2.99s\n",
      "    154       \u001b[36m1.12061\u001b[0m       \u001b[32m0.87134\u001b[0m      1.28608      0.71859  3.04s\n",
      "    155       \u001b[36m1.11405\u001b[0m       0.87701      1.27029      0.71781  2.95s\n",
      "    156       \u001b[36m1.11226\u001b[0m       0.87190      1.27567      0.71880  2.98s\n",
      "    157       1.11587       0.87499      1.27530      0.71650  3.02s\n",
      "    158       \u001b[36m1.09864\u001b[0m       0.87341      1.25787      0.71700  3.03s\n",
      "    159       1.11054       0.87406      1.27056      0.71694  3.03s\n",
      "    160       1.10325       \u001b[32m0.86801\u001b[0m      1.27100      0.71925  3.03s\n",
      "    161       1.10781       \u001b[32m0.86740\u001b[0m      1.27716      0.71920  3.07s\n",
      "    162       1.10397       \u001b[32m0.86588\u001b[0m      1.27497      0.72013  3.00s\n",
      "    163       1.10687       \u001b[32m0.86311\u001b[0m      1.28241      0.72119  2.96s\n",
      "    164       1.10991       0.86745      1.27951      0.71852  3.02s\n",
      "    165       \u001b[36m1.09633\u001b[0m       0.86365      1.26942      0.72023  3.04s\n",
      "    166       1.09706       \u001b[32m0.85997\u001b[0m      1.27569      0.72217  3.02s\n",
      "    167       \u001b[36m1.09415\u001b[0m       0.86337      1.26729      0.72041  2.92s\n",
      "    168       1.09525       0.86100      1.27206      0.72127  3.03s\n",
      "    169       1.09529       \u001b[32m0.85297\u001b[0m      1.28409      0.72432  2.98s\n",
      "    170       \u001b[36m1.09346\u001b[0m       0.85933      1.27246      0.72050  3.05s\n",
      "    171       \u001b[36m1.08814\u001b[0m       0.86424      1.25907      0.71866  3.00s\n",
      "    172       1.08852       0.85877      1.26754      0.72202  2.95s\n",
      "    173       1.08824       0.85803      1.26830      0.71991  2.89s\n",
      "    174       \u001b[36m1.07696\u001b[0m       0.85830      1.25475      0.72197  2.95s\n",
      "    175       1.07882       \u001b[32m0.84724\u001b[0m      1.27334      0.72668  2.99s\n",
      "    176       1.07904       0.85281      1.26528      0.72345  3.04s\n",
      "    177       1.07981       0.85319      1.26561      0.72249  3.04s\n",
      "    178       \u001b[36m1.07668\u001b[0m       0.85102      1.26516      0.72499  2.95s\n",
      "    179       \u001b[36m1.07598\u001b[0m       0.85394      1.26003      0.72257  3.02s\n",
      "    180       \u001b[36m1.07487\u001b[0m       0.85053      1.26377      0.72406  3.00s\n",
      "    181       1.07547       0.85458      1.25847      0.72169  2.97s\n",
      "    182       \u001b[36m1.06837\u001b[0m       0.85232      1.25349      0.72235  2.98s\n",
      "    183       1.07666       \u001b[32m0.84584\u001b[0m      1.27289      0.72351  3.02s\n",
      "    184       1.06926       0.84987      1.25815      0.72248  3.09s\n",
      "    185       \u001b[36m1.06246\u001b[0m       0.84998      1.24998      0.72094  3.13s\n",
      "    186       \u001b[36m1.05918\u001b[0m       0.84622      1.25166      0.72444  3.12s\n",
      "    187       1.06602       0.84811      1.25693      0.72352  2.98s\n",
      "    188       1.06056       \u001b[32m0.84034\u001b[0m      1.26205      0.72857  3.14s\n",
      "    189       \u001b[36m1.05865\u001b[0m       0.85580      1.23704      0.71851  3.13s\n",
      "    190       1.06237       \u001b[32m0.83344\u001b[0m      1.27467      0.72946  3.12s\n",
      "    191       1.06005       0.83910      1.26332      0.72600  2.99s\n",
      "    192       \u001b[36m1.05657\u001b[0m       0.84851      1.24520      0.72134  3.01s\n",
      "    193       \u001b[36m1.05494\u001b[0m       0.84363      1.25048      0.72253  3.12s\n",
      "    194       \u001b[36m1.04756\u001b[0m       0.83919      1.24830      0.72611  2.99s\n",
      "    195       1.05111       \u001b[32m0.83120\u001b[0m      1.26457      0.72974  3.04s\n",
      "    196       1.04820       0.83391      1.25697      0.72660  2.96s\n",
      "    197       1.05270       0.83604      1.25915      0.72629  2.98s\n",
      "    198       \u001b[36m1.04479\u001b[0m       0.83702      1.24822      0.72660  3.02s\n",
      "    199       \u001b[36m1.04198\u001b[0m       \u001b[32m0.82881\u001b[0m      1.25721      0.72869  2.96s\n",
      "    200       \u001b[36m1.04183\u001b[0m       0.84401      1.23438      0.72253  3.01s\n",
      "    201       \u001b[36m1.03941\u001b[0m       0.83210      1.24914      0.72831  2.98s\n",
      "    202       1.04088       0.83313      1.24936      0.72601  2.95s\n",
      "    203       \u001b[36m1.03344\u001b[0m       0.83877      1.23210      0.72532  2.97s\n",
      "    204       1.03550       0.83271      1.24353      0.72671  2.94s\n",
      "    205       1.03519       \u001b[32m0.82507\u001b[0m      1.25467      0.73078  2.98s\n",
      "    206       \u001b[36m1.03335\u001b[0m       0.82774      1.24840      0.72825  3.02s\n",
      "    207       \u001b[36m1.02380\u001b[0m       0.83529      1.22569      0.72606  3.00s\n",
      "    208       1.03622       0.83666      1.23852      0.72748  3.09s\n",
      "    209       \u001b[36m1.02276\u001b[0m       0.82797      1.23525      0.73066  3.06s\n",
      "    210       1.02901       0.83084      1.23852      0.72819  2.91s\n",
      "    211       1.02421       \u001b[32m0.82504\u001b[0m      1.24140      0.72975  2.93s\n",
      "    212       1.03033       \u001b[32m0.82194\u001b[0m      1.25354      0.73155  2.94s\n",
      "    213       1.02641       0.82271      1.24760      0.73232  3.00s\n",
      "    214       \u001b[36m1.01272\u001b[0m       0.83112      1.21851      0.72777  2.98s\n",
      "    215       1.02319       0.83157      1.23043      0.72625  2.93s\n",
      "    216       1.01817       0.82632      1.23217      0.73058  2.97s\n",
      "    217       1.01951       0.82434      1.23676      0.73065  2.91s\n",
      "    218       \u001b[36m1.01142\u001b[0m       \u001b[32m0.81664\u001b[0m      1.23851      0.73066  2.94s\n",
      "    219       1.01706       \u001b[32m0.81623\u001b[0m      1.24604      0.73316  2.95s\n",
      "    220       1.01323       0.82551      1.22740      0.72759  2.98s\n",
      "    221       \u001b[36m1.00896\u001b[0m       0.81978      1.23078      0.73067  2.91s\n",
      "    222       1.01630       0.82298      1.23491      0.72868  2.95s\n",
      "    223       \u001b[36m1.00361\u001b[0m       0.81693      1.22851      0.73183  2.94s\n",
      "    224       1.00496       \u001b[32m0.81262\u001b[0m      1.23669      0.73463  2.92s\n",
      "    225       1.00430       0.81310      1.23515      0.73315  2.96s\n",
      "    226       \u001b[36m1.00070\u001b[0m       0.81800      1.22335      0.73116  2.97s\n",
      "    227       1.00090       0.81613      1.22640      0.73243  2.91s\n",
      "    228       1.00118       \u001b[32m0.80982\u001b[0m      1.23631      0.73526  2.92s\n",
      "    229       \u001b[36m0.99828\u001b[0m       0.81620      1.22308      0.73217  2.91s\n",
      "    230       \u001b[36m0.99445\u001b[0m       0.81834      1.21520      0.73158  2.95s\n",
      "    231       1.00032       0.81297      1.23046      0.73293  2.93s\n",
      "    232       0.99860       0.81251      1.22903      0.73321  2.93s\n",
      "    233       \u001b[36m0.99255\u001b[0m       0.81059      1.22448      0.73520  2.87s\n",
      "    234       1.00039       \u001b[32m0.80160\u001b[0m      1.24798      0.73911  3.04s\n",
      "    235       \u001b[36m0.99185\u001b[0m       0.80673      1.22947      0.73636  2.98s\n",
      "    236       \u001b[36m0.99158\u001b[0m       0.81273      1.22007      0.73329  2.94s\n",
      "    237       \u001b[36m0.98933\u001b[0m       0.80783      1.22467      0.73551  2.98s\n",
      "    238       \u001b[36m0.98730\u001b[0m       0.81340      1.21379      0.73065  3.01s\n",
      "    239       \u001b[36m0.98719\u001b[0m       0.80766      1.22228      0.73595  2.95s\n",
      "    240       \u001b[36m0.98262\u001b[0m       0.81162      1.21070      0.73301  2.98s\n",
      "    241       0.98861       0.80731      1.22457      0.73602  3.09s\n",
      "    242       \u001b[36m0.97982\u001b[0m       0.80912      1.21097      0.73342  2.94s\n",
      "    243       \u001b[36m0.97031\u001b[0m       0.80526      1.20497      0.73555  3.00s\n",
      "    244       0.98208       0.80379      1.22181      0.73903  2.97s\n",
      "    245       0.97710       0.80665      1.21130      0.73589  3.04s\n",
      "    246       0.97317       0.80763      1.20497      0.73625  2.99s\n",
      "    247       0.97133       \u001b[32m0.80112\u001b[0m      1.21246      0.73771  2.93s\n",
      "    248       \u001b[36m0.97020\u001b[0m       0.80184      1.20997      0.73646  2.96s\n",
      "    249       \u001b[36m0.96613\u001b[0m       0.80182      1.20492      0.73948  2.96s\n",
      "    250       0.96887       \u001b[32m0.79981\u001b[0m      1.21137      0.73746  2.99s\n",
      "    251       0.97367       0.80343      1.21188      0.73646  2.97s\n",
      "    252       0.96822       0.80133      1.20827      0.73711  3.01s\n",
      "    253       0.96812       0.80306      1.20555      0.73588  2.99s\n",
      "    254       \u001b[36m0.96590\u001b[0m       \u001b[32m0.79314\u001b[0m      1.21781      0.74153  2.97s\n",
      "    255       \u001b[36m0.96393\u001b[0m       0.80565      1.19645      0.73453  2.94s\n",
      "    256       \u001b[36m0.96066\u001b[0m       0.79848      1.20311      0.73761  2.93s\n",
      "    257       0.96082       0.79658      1.20617      0.74057  2.98s\n",
      "    258       \u001b[36m0.95463\u001b[0m       0.79873      1.19519      0.73892  2.98s\n",
      "    259       0.96044       0.80188      1.19773      0.73594  2.99s\n",
      "    260       0.95621       \u001b[32m0.78608\u001b[0m      1.21644      0.74455  3.04s\n",
      "    261       0.95792       0.79772      1.20083      0.73748  2.93s\n",
      "    262       \u001b[36m0.94548\u001b[0m       0.79319      1.19200      0.73910  2.97s\n",
      "    263       0.95089       0.80079      1.18745      0.73737  2.98s\n",
      "    264       0.94759       0.79281      1.19523      0.73984  3.08s\n",
      "    265       0.95452       0.79361      1.20276      0.74052  3.02s\n",
      "    266       0.95208       0.79551      1.19681      0.73839  3.01s\n",
      "    267       0.95011       0.79450      1.19586      0.74039  2.96s\n",
      "    268       0.94648       0.79528      1.19012      0.73817  2.99s\n",
      "    269       0.94625       0.79022      1.19746      0.73983  2.98s\n",
      "    270       \u001b[36m0.94274\u001b[0m       0.79547      1.18514      0.74000  2.98s\n",
      "    271       0.94577       0.79632      1.18767      0.73721  2.98s\n",
      "    272       \u001b[36m0.93646\u001b[0m       0.78816      1.18816      0.74142  2.94s\n",
      "    273       0.94258       0.79330      1.18817      0.73837  3.08s\n",
      "    274       \u001b[36m0.93457\u001b[0m       0.78711      1.18735      0.74131  3.00s\n",
      "    275       0.94584       0.79184      1.19449      0.74191  2.96s\n",
      "    276       0.93910       0.78744      1.19260      0.74152  2.99s\n",
      "    277       \u001b[36m0.92782\u001b[0m       0.79553      1.16630      0.73796  2.94s\n",
      "    278       0.93452       0.78711      1.18728      0.74072  3.01s\n",
      "    279       0.93655       0.79261      1.18160      0.74029  3.01s\n",
      "    280       0.93135       0.79038      1.17836      0.73815  2.99s\n",
      "    281       0.93594       0.79193      1.18184      0.73954  3.03s\n",
      "    282       0.93077       0.79251      1.17445      0.73773  3.02s\n",
      "    283       0.92881       \u001b[32m0.78582\u001b[0m      1.18197      0.74084  2.97s\n",
      "    284       \u001b[36m0.92481\u001b[0m       0.78953      1.17135      0.74050  2.96s\n",
      "    285       \u001b[36m0.91937\u001b[0m       0.78977      1.16409      0.74007  2.99s\n",
      "    286       0.92804       0.79042      1.17411      0.74105  3.04s\n",
      "    287       0.92660       0.78827      1.17549      0.73996  3.05s\n",
      "    288       0.92179       0.79282      1.16268      0.73930  2.99s\n",
      "    289       0.92029       \u001b[32m0.78517\u001b[0m      1.17210      0.74218  3.04s\n",
      "    290       0.92789       0.79073      1.17346      0.73820  3.07s\n",
      "    291       \u001b[36m0.91660\u001b[0m       \u001b[32m0.78088\u001b[0m      1.17380      0.74555  3.01s\n",
      "    292       0.91971       \u001b[32m0.77979\u001b[0m      1.17943      0.74275  3.02s\n",
      "    293       \u001b[36m0.91266\u001b[0m       \u001b[32m0.77894\u001b[0m      1.17167      0.74329  3.04s\n",
      "    294       0.91522       0.78631      1.16394      0.74152  3.00s\n",
      "    295       0.92012       \u001b[32m0.77609\u001b[0m      1.18559      0.74586  3.02s\n",
      "    296       \u001b[36m0.90903\u001b[0m       0.77925      1.16653      0.74123  3.00s\n",
      "    297       \u001b[36m0.90685\u001b[0m       0.77657      1.16776      0.74557  3.05s\n",
      "    298       0.91284       0.78786      1.15863      0.74144  3.22s\n",
      "    299       0.90938       \u001b[32m0.77414\u001b[0m      1.17470      0.74469  2.99s\n",
      "    300       \u001b[36m0.90683\u001b[0m       0.77539      1.16951      0.74350  3.09s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NeuralNet(X_tensor_type=None,\n",
       "     batch_iterator_test=<nolearn.lasagne.base.BatchIterator object at 0x7ff2981c11d0>,\n",
       "     batch_iterator_train=<nolearn.lasagne.base.BatchIterator object at 0x7ff2981c1150>,\n",
       "     custom_score=None, dropout1_p=0.2, dropout2_p=0.2, dropout_p=0.2,\n",
       "     hidden1_num_units=128, hidden2_num_units=64, input_shape=(None, 7079),\n",
       "     layers=[('input', <class 'lasagne.layers.input.InputLayer'>), ('dropout', <class 'lasagne.layers.noise.DropoutLayer'>), ('hidden1', <class 'lasagne.layers.dense.DenseLayer'>), ('dropout1', <class 'lasagne.layers.noise.DropoutLayer'>), ('hidden2', <class 'lasagne.layers.dense.DenseLayer'>), ('dropout2', <class 'lasagne.layers.noise.DropoutLayer'>), ('output', <class 'lasagne.layers.dense.DenseLayer'>)],\n",
       "     loss=None, max_epochs=300, more_params={},\n",
       "     objective=<function objective at 0x7ff2981b3488>,\n",
       "     objective_loss_function=<function categorical_crossentropy at 0x7ff29820b0c8>,\n",
       "     on_epoch_finished=[<nolearn.lasagne.handlers.PrintLog instance at 0x7ff16f559ab8>],\n",
       "     on_training_finished=[],\n",
       "     on_training_started=[<nolearn.lasagne.handlers.PrintLayerInfo instance at 0x7ff16f559998>],\n",
       "     output_nonlinearity=<function softmax at 0x7ff298703848>,\n",
       "     output_num_units=38, regression=False,\n",
       "     train_split=<nolearn.lasagne.base.TrainSplit object at 0x7ff2981c1210>,\n",
       "     update=<function nesterov_momentum at 0x7ff29820b848>,\n",
       "     update_learning_rate=0.0005, update_momentum=0.9,\n",
       "     use_label_encoder=False, verbose=True,\n",
       "     y_tensor_type=TensorType(int32, vector))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_probas = nn.predict_proba(X_test)\n",
    "\n",
    "\n",
    "col_names = ['TripType_' + str(c) for c in enc.classes_.astype('int')]\n",
    "submission = pd.DataFrame(np.round(y_probas, 4), index=y_df[pd.isnull(y_df.TripType)].index, columns = col_names)\n",
    "\n",
    "submission.reset_index(inplace = True)\n",
    "submission.to_csv('Walmart_submission_NN_7000Features-6-2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##TODO  removing train test split and use all training data for NN\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train_preditions = nn.predict_proba(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "log1 = LogisticRegression(C=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90674, 38)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_preditions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, penalty='l2', random_state=None, tol=0.0001)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log1.fit(y_train_preditions, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82604715795045991"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log1.score(y_train_preditions,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_logistic_probas = log1.predict_proba(y_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

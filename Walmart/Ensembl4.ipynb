{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GRID K520 (CNMeM is disabled)\n"
     ]
    }
   ],
   "source": [
    "# %load Walmart_Ensembl4.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.decomposition import NMF\n",
    "import xgboost\n",
    "\n",
    "import theano\n",
    "from lasagne import layers, nonlinearities\n",
    "from nolearn.lasagne import NeuralNet, BatchIterator\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#import sys\n",
    "\n",
    "#submission_name = sys.argv[0].split('.')[0]\n",
    "#print 'Generating Submission: ' + submission_name\n",
    "\n",
    "train = pd.read_csv('./train.csv') #Last visit number is 191347\n",
    "test = pd.read_csv('./test.csv') #Last visit number is 191348\n",
    "\n",
    "full_df = pd.concat((train, test))\n",
    "\n",
    "full_df_negatives = full_df[full_df.ScanCount < 0]\n",
    "full_df_negatives_agg = full_df_negatives.groupby(['VisitNumber']).agg({'ScanCount':np.sum}) #Negative Feature Count\n",
    "\n",
    "full_df_uncategorized = full_df[pd.isnull(full_df.Upc)]\n",
    "full_df_uncategorized_agg = full_df_uncategorized.groupby(['VisitNumber']).agg({'ScanCount':np.sum}) #Unknown Feature Count\n",
    "\n",
    "full_df_totals = full_df[full_df.ScanCount > 0]\n",
    "full_df_totals_agg = full_df_totals.groupby(['VisitNumber']).agg({'ScanCount':np.sum}) #Total purchases Feature Count\n",
    "\n",
    "\n",
    "full_df.Upc.fillna(-100, inplace=True)\n",
    "full_df.DepartmentDescription.fillna('UNKNOWN', inplace=True)\n",
    "full_df.FinelineNumber.fillna(-100, inplace=True)\n",
    "\n",
    "visit_days = full_df.loc[:,['VisitNumber','Weekday']]\n",
    "visit_days.drop_duplicates('VisitNumber', inplace = True)\n",
    "visit_days.set_index('VisitNumber', inplace = True)\n",
    "visit_days = pd.get_dummies(visit_days) #Generating an interaction term with visit days would be useful\n",
    "\n",
    "full_df['FinelineNumber'] = full_df['FinelineNumber'].astype('int')\n",
    "full_df['DeptItems'] = full_df.DepartmentDescription +' ' + full_df.FinelineNumber.astype('str')\n",
    "\n",
    "\n",
    "full_df.loc[full_df.ScanCount < 0, 'DeptItems'] = '-' + full_df.loc[full_df.ScanCount < 0, :].DeptItems\n",
    "full_df['ScanCount'] = np.abs(full_df.ScanCount)\n",
    "full_df['ScanCount'] = full_df.ScanCount.astype('float32')\n",
    "\n",
    "full_deptitems_df = pd.pivot_table(full_df[full_df.ScanCount>0], values='ScanCount', index='VisitNumber',columns='DeptItems', aggfunc=np.sum)\n",
    "full_deptitems_df.fillna(0, inplace=True)\n",
    "\n",
    "y_df = full_df.loc[:, ['VisitNumber', 'TripType']]\n",
    "y_df.drop_duplicates('VisitNumber', inplace=True)\n",
    "y_df.set_index('VisitNumber', inplace=True)\n",
    "\n",
    "y_df = y_df.join(full_deptitems_df) #This requires an insane amount of memory **Cannot fill 0s due to memory error\n",
    "\n",
    "del full_deptitems_df\n",
    "\n",
    "X_train = y_df[pd.notnull(y_df.TripType)].drop('TripType', axis = 1).values\n",
    "X_test = y_df[pd.isnull(y_df.TripType)].drop('TripType', axis = 1).values\n",
    "y_train = y_df[pd.notnull(y_df.TripType)]['TripType'].values\n",
    "\n",
    "\n",
    "y_df = y_df[['TripType']] #Removing Unneccessary Columns\n",
    "\n",
    "X_train = np.nan_to_num(X_train) #Splitting this into 2 cells works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "visit_days_interaction = visit_days.join(full_df_totals_agg)\n",
    "visit_days_interaction.fillna(value = 0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for c in visit_days_interaction:\n",
    "    if 'Weekday' in c:\n",
    "        visit_days_interaction[c] = visit_days_interaction[c] * visit_days_interaction.ScanCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_df = pd.pivot_table(full_df, values='ScanCount', index='VisitNumber',columns='DepartmentDescription', aggfunc=np.sum)\n",
    "X_df.fillna(0, inplace=True)\n",
    "\n",
    "X_df = X_df.join(full_df_totals_agg, rsuffix='Totals')\n",
    "X_df = X_df.join(full_df_uncategorized_agg, rsuffix='Uncategorized')\n",
    "X_df = X_df.join(full_df_negatives_agg, rsuffix='Negatives')\n",
    "X_df = X_df.join(visit_days)\n",
    "X_df = X_df.join(visit_days_interaction,rsuffix='interaction')\n",
    "X_df.fillna(0, inplace = True)\n",
    "\n",
    "y_df = y_df.join(X_df)\n",
    "\n",
    "X_train2 = y_df[pd.notnull(y_df.TripType)].drop('TripType', axis = 1).values\n",
    "X_test2 = y_df[pd.isnull(y_df.TripType)].drop('TripType', axis = 1).values\n",
    "y_train2 = y_df[pd.notnull(y_df.TripType)]['TripType'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(95674, 17710)\n",
      "(95674, 17710)\n",
      "(95674, 87)\n",
      "(95674, 87)\n"
     ]
    }
   ],
   "source": [
    "print X_train.shape\n",
    "print X_test.shape\n",
    "print X_train2.shape\n",
    "print X_test2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "zeros_index_train = np.sum(X_train, axis = 0) == 0 #2389 zeros in array\n",
    "zeros_index_test = np.sum(X_test, axis = 0) == 0 #2310 zeros in array\n",
    "zeros_index = zeros_index_train | zeros_index_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = X_train[:,~zeros_index]\n",
    "X_test = X_test[:,~zeros_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(95674, 13011)\n",
      "(95674, 13011)\n"
     ]
    }
   ],
   "source": [
    "print X_test.shape\n",
    "print X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%timeit\n",
    "nmf = NMF(n_components = 10000, max_iter=2)\n",
    "X_train = nmf.fit_transform(X_train)\n",
    "\n",
    "X_test = np.nan_to_num(X_test)\n",
    "X_test = nmf.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = np.concatenate((X_train, X_train2), axis = 1)\n",
    "X_test = np.concatenate((X_test, X_test2), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "enc = LabelEncoder()\n",
    "y_train = enc.fit_transform(y_train)\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "y_train = y_train.astype('int32')\n",
    "\n",
    "class AdjustVariable(object):\n",
    "    def __init__(self, name, start=0.03, stop=0.001):\n",
    "        self.name = name\n",
    "        self.start, self.stop = start, stop\n",
    "        self.ls = None\n",
    "\n",
    "    def __call__(self, nn, train_history):\n",
    "        if self.ls is None:\n",
    "            self.ls = np.linspace(self.start, self.stop, nn.max_epochs)\n",
    "\n",
    "        epoch = train_history[-1]['epoch']\n",
    "        new_value = float32(self.ls[epoch - 1])\n",
    "        getattr(nn, self.name).set_value(new_value)\n",
    "        \n",
    "class EarlyStopping(object):\n",
    "    def __init__(self, patience=100):\n",
    "        self.patience = patience\n",
    "        self.best_valid = np.inf\n",
    "        self.best_valid_epoch = 0\n",
    "        self.best_weights = None\n",
    "\n",
    "    def __call__(self, nn, train_history):\n",
    "        current_valid = train_history[-1]['valid_loss']\n",
    "        current_epoch = train_history[-1]['epoch']\n",
    "        if current_valid < self.best_valid:\n",
    "            self.best_valid = current_valid\n",
    "            self.best_valid_epoch = current_epoch\n",
    "            self.best_weights = nn.get_all_params_values()\n",
    "        elif self.best_valid_epoch + self.patience < current_epoch:\n",
    "            print(\"Early stopping.\")\n",
    "            print(\"Best valid loss was {:.6f} at epoch {}.\".format(\n",
    "                self.best_valid, self.best_valid_epoch))\n",
    "            nn.load_params_from(self.best_weights)\n",
    "            raise StopIteration()\n",
    "            \n",
    "def float32(k):\n",
    "    return np.cast['float32'](k)\n",
    "\n",
    "nn = NeuralNet(layers = [\n",
    "     ('input', layers.InputLayer),\n",
    "     ('dropout', layers.DropoutLayer),\n",
    "     ('hidden1', layers.DenseLayer),\n",
    "     ('dropout1', layers.DropoutLayer),   \n",
    "     ('hidden2', layers.DenseLayer),\n",
    "     ('dropout2', layers.DropoutLayer),   \n",
    "     ('output', layers.DenseLayer),],\n",
    "               \n",
    "     input_shape = (None, X_train.shape[1]),\n",
    "     dropout_p =.20,\n",
    "               \n",
    "     hidden1_num_units = 256,\n",
    "     dropout1_p = .25,\n",
    "     hidden2_num_units = 128,\n",
    "     dropout2_p = .25,\n",
    "               \n",
    "     output_num_units = np.unique(y_train).shape[0],\n",
    "     output_nonlinearity = nonlinearities.softmax,\n",
    "               \n",
    "     update_learning_rate=theano.shared(float32(0.01)),\n",
    "     update_momentum=theano.shared(float32(0.9)),\n",
    "    \n",
    "     batch_iterator_train=BatchIterator(batch_size=2048),\n",
    "               \n",
    "     on_epoch_finished=[\n",
    "        AdjustVariable('update_learning_rate', start=0.01, stop=0.0001),\n",
    "        AdjustVariable('update_momentum', start=0.9, stop=0.999),\n",
    "        EarlyStopping(patience=25)\n",
    "        ],\n",
    "\n",
    "     regression = False,\n",
    "     max_epochs = 5,\n",
    "     verbose = True\n",
    "      )\n",
    "\n",
    "nn.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_, X_val, y_, y_val = train_test_split(X_train, y_train, test_size = 25000, random_state = 13)\n",
    "\n",
    "del X_\n",
    "del y_\n",
    "\n",
    "xgb = xgboost.XGBClassifier(max_depth = 14, n_estimators = 5,\n",
    "                        objective='multi:softprob', subsample = .80, colsample_bytree=.5)\n",
    "\n",
    "xgb.fit(X_train, y_train, eval_set = [(X_val, y_val)], eval_metric = 'mlogloss', early_stopping_rounds=25)\n",
    "\n",
    "y_xgb_train_predictions = xgb.predict_proba(X_train)\n",
    "y_nn_train_predictions = nn.predict_proba(X_train)\n",
    "\n",
    "X_ensembl_train = np.concatenate((y_xgb_train_predictions, y_nn_train_predictions), axis = 1)\n",
    "\n",
    "y_nn_test_predictions = nn.predict_proba(X_test)\n",
    "y_xgb_test_predictions = xgb.predict_proba(X_test)\n",
    "\n",
    "\n",
    "col_names = ['TripType_' + str(c) for c in enc.classes_.astype('int')]\n",
    "\n",
    "submission = pd.DataFrame(np.round(y_nn_test_predictions, 4), index=y_df[pd.isnull(y_df.TripType)].index, columns = col_names)\n",
    "submission.reset_index(inplace = True)\n",
    "submission.to_csv(submission_name + '_NN_10000Features.csv', index=False)\n",
    "\n",
    "submission = pd.DataFrame(np.round(y_xgb_test_predictions, 4), index=y_df[pd.isnull(y_df.TripType)].index, columns = col_names)\n",
    "submission.reset_index(inplace = True)\n",
    "submission.to_csv(submission_name + '_XGB_10000Features.csv', index=False)\n",
    "\n",
    "log_ensembl = LogisticRegression()\n",
    "log_ensembl.fit(X_ensembl_train, y_train)\n",
    "X_ensembl_test = np.concatenate((y_xgb_test_predictions, y_nn_test_predictions), axis = 1)\n",
    "\n",
    "y_probas = log_ensembl.predict_proba(X_ensembl_test)\n",
    "\n",
    "submission = pd.DataFrame(np.round(y_probas, 4), index=y_df[pd.isnull(y_df.TripType)].index, columns = col_names)\n",
    "submission.reset_index(inplace = True)\n",
    "submission.to_csv(submission_name + '_log_ensembl_10000Features-Notebook.csv', index=False)\n",
    "\n",
    "y_probas_avg = (y_xgb_test_predictions + y_nn_test_predictions)/2\n",
    "\n",
    "submission = pd.DataFrame(np.round(y_probas_avg,4), index=y_df[pd.isnull(y_df.TripType)].index, columns = col_names)\n",
    "submission.reset_index(inplace = True)\n",
    "submission.to_csv(submission_name + '_avg_ensembl_10000Features-Notebook.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

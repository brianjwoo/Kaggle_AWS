{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GRID K520 (CNMeM is disabled)\n"
     ]
    }
   ],
   "source": [
    "# %load Walmart-NN-6.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import xgboost\n",
    "\n",
    "import theano\n",
    "from lasagne import layers, nonlinearities\n",
    "from nolearn.lasagne import NeuralNet, BatchIterator\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('./train.csv') #Last visit number is 191347\n",
    "test = pd.read_csv('./test.csv') #Last visit number is 191348\n",
    "\n",
    "full_df = pd.concat((train, test))\n",
    "\n",
    "full_df_negatives = full_df[full_df.ScanCount < 0]\n",
    "full_df_negatives_agg = full_df_negatives.groupby(['VisitNumber']).agg({'ScanCount':np.sum}) #Negative Feature Count\n",
    "\n",
    "full_df_uncategorized = full_df[pd.isnull(full_df.Upc)]\n",
    "full_df_uncategorized_agg = full_df_uncategorized.groupby(['VisitNumber']).agg({'ScanCount':np.sum}) #Unknown Feature Count\n",
    "\n",
    "full_df_totals = full_df[full_df.ScanCount > 0]\n",
    "full_df_totals_agg = full_df_totals.groupby(['VisitNumber']).agg({'ScanCount':np.sum}) #Total purchases Feature Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DepartmentDescription</th>\n",
       "      <th>FinelineNumber</th>\n",
       "      <th>ScanCount</th>\n",
       "      <th>TripType</th>\n",
       "      <th>Upc</th>\n",
       "      <th>VisitNumber</th>\n",
       "      <th>Weekday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FINANCIAL SERVICES</td>\n",
       "      <td>1000</td>\n",
       "      <td>-1</td>\n",
       "      <td>999</td>\n",
       "      <td>68113152929</td>\n",
       "      <td>5</td>\n",
       "      <td>Friday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SHOES</td>\n",
       "      <td>8931</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>60538815980</td>\n",
       "      <td>7</td>\n",
       "      <td>Friday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PERSONAL CARE</td>\n",
       "      <td>4504</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>7410811099</td>\n",
       "      <td>7</td>\n",
       "      <td>Friday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PAINT AND ACCESSORIES</td>\n",
       "      <td>3565</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>2238403510</td>\n",
       "      <td>8</td>\n",
       "      <td>Friday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PAINT AND ACCESSORIES</td>\n",
       "      <td>1017</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>2006613744</td>\n",
       "      <td>8</td>\n",
       "      <td>Friday</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   DepartmentDescription  FinelineNumber  ScanCount  TripType          Upc  \\\n",
       "0     FINANCIAL SERVICES            1000         -1       999  68113152929   \n",
       "1                  SHOES            8931          1        30  60538815980   \n",
       "2          PERSONAL CARE            4504          1        30   7410811099   \n",
       "3  PAINT AND ACCESSORIES            3565          2        26   2238403510   \n",
       "4  PAINT AND ACCESSORIES            1017          2        26   2006613744   \n",
       "\n",
       "   VisitNumber Weekday  \n",
       "0            5  Friday  \n",
       "1            7  Friday  \n",
       "2            7  Friday  \n",
       "3            8  Friday  \n",
       "4            8  Friday  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print full_df.FinelineNumber.nunique() #5353\n",
    "#print full_df.Upc.nunique() #124693"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#full_df[pd.isnull(full_df.FinelineNumber)].tail(30) #Most Values that have a NA for FinelineNumber Also have NA for Upc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_df.Upc.fillna(-100, inplace=True)\n",
    "full_df.DepartmentDescription.fillna('UNKNOWN', inplace=True)\n",
    "full_df.FinelineNumber.fillna(-100, inplace=True)\n",
    "\n",
    "visit_days = full_df.loc[:,['VisitNumber','Weekday']]\n",
    "visit_days.drop_duplicates('VisitNumber', inplace = True)\n",
    "visit_days.set_index('VisitNumber', inplace = True)\n",
    "visit_days = pd.get_dummies(visit_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "full_df['FinelineNumber'] = full_df['FinelineNumber'].astype('int')\n",
    "full_df['DeptItems'] = full_df.DepartmentDescription +' ' + full_df.FinelineNumber.astype('str')\n",
    "\n",
    "full_deptitems_df = pd.pivot_table(full_df[full_df.ScanCount>0], values='ScanCount', index='VisitNumber',columns='DeptItems', aggfunc=np.sum)\n",
    "full_deptitems_df.fillna(0, inplace=True)\n",
    "\n",
    "\n",
    "y_df = full_df.loc[:, ['VisitNumber', 'TripType']]\n",
    "y_df.drop_duplicates('VisitNumber', inplace=True)\n",
    "y_df.set_index('VisitNumber', inplace=True)\n",
    "\n",
    "y_df = y_df.join(full_deptitems_df) #This requires an insane amount of memory **Cannot fill 0s due to memory error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "del full_deptitems_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = y_df[pd.notnull(y_df.TripType)].drop('TripType', axis = 1).values\n",
    "X_test = y_df[pd.isnull(y_df.TripType)].drop('TripType', axis = 1).values\n",
    "y_train = y_df[pd.notnull(y_df.TripType)]['TripType'].values\n",
    "\n",
    "\n",
    "y_df = y_df[['TripType']] #Removing Unneccessary Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = np.nan_to_num(X_train) #Splitting this into 2 cells works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "chi_sq_best = SelectKBest(score_func=chi2, k = 10000)\n",
    "chi_sq_best.fit(X_train,y_train)\n",
    "\n",
    "X_train = chi_sq_best.transform(X_train)\n",
    "\n",
    "X_test = np.nan_to_num(X_test)\n",
    "X_test = chi_sq_best.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_df = pd.pivot_table(full_df, values='ScanCount', index='VisitNumber',columns='DepartmentDescription', aggfunc=np.sum)\n",
    "X_df.fillna(0, inplace=True)\n",
    "\n",
    "\n",
    "X_df = X_df.join(full_df_totals_agg, rsuffix='Totals')\n",
    "X_df = X_df.join(full_df_uncategorized_agg, rsuffix='Uncategorized')\n",
    "X_df = X_df.join(full_df_negatives_agg, rsuffix='Negatives')\n",
    "X_df = X_df.join(visit_days)\n",
    "X_df.fillna(0, inplace = True)\n",
    "\n",
    "y_df = y_df.join(X_df)\n",
    "\n",
    "X_train2 = y_df[pd.notnull(y_df.TripType)].drop('TripType', axis = 1).values\n",
    "X_test2 = y_df[pd.isnull(y_df.TripType)].drop('TripType', axis = 1).values\n",
    "y_train2 = y_df[pd.notnull(y_df.TripType)]['TripType'].values\n",
    "\n",
    "X_train = np.concatenate((X_train, X_train2), axis = 1)\n",
    "X_test = np.concatenate((X_test, X_test2), axis = 1)\n",
    "\n",
    "enc = LabelEncoder()\n",
    "y_train = enc.fit_transform(y_train)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 5000, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "y_train = y_train.astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AdjustVariable(object):\n",
    "    def __init__(self, name, start=0.03, stop=0.001):\n",
    "        self.name = name\n",
    "        self.start, self.stop = start, stop\n",
    "        self.ls = None\n",
    "\n",
    "    def __call__(self, nn, train_history):\n",
    "        if self.ls is None:\n",
    "            self.ls = np.linspace(self.start, self.stop, nn.max_epochs)\n",
    "\n",
    "        epoch = train_history[-1]['epoch']\n",
    "        new_value = float32(self.ls[epoch - 1])\n",
    "        getattr(nn, self.name).set_value(new_value)\n",
    "        \n",
    "class EarlyStopping(object):\n",
    "    def __init__(self, patience=100):\n",
    "        self.patience = patience\n",
    "        self.best_valid = np.inf\n",
    "        self.best_valid_epoch = 0\n",
    "        self.best_weights = None\n",
    "\n",
    "    def __call__(self, nn, train_history):\n",
    "        current_valid = train_history[-1]['valid_loss']\n",
    "        current_epoch = train_history[-1]['epoch']\n",
    "        if current_valid < self.best_valid:\n",
    "            self.best_valid = current_valid\n",
    "            self.best_valid_epoch = current_epoch\n",
    "            self.best_weights = nn.get_all_params_values()\n",
    "        elif self.best_valid_epoch + self.patience < current_epoch:\n",
    "            print(\"Early stopping.\")\n",
    "            print(\"Best valid loss was {:.6f} at epoch {}.\".format(\n",
    "                self.best_valid, self.best_valid_epoch))\n",
    "            nn.load_params_from(self.best_weights)\n",
    "            raise StopIteration()\n",
    "            \n",
    "def float32(k):\n",
    "    return np.cast['float32'](k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nn = NeuralNet(layers = [\n",
    "     ('input', layers.InputLayer),\n",
    "     ('dropout', layers.DropoutLayer),\n",
    "     ('hidden1', layers.DenseLayer),\n",
    "     ('dropout1', layers.DropoutLayer),   \n",
    "     ('hidden2', layers.DenseLayer),\n",
    "     ('dropout2', layers.DropoutLayer),   \n",
    "     ('output', layers.DenseLayer),],\n",
    "               \n",
    "     input_shape = (None, X_train.shape[1]),\n",
    "     dropout_p =.25,\n",
    "               \n",
    "     hidden1_num_units = 128,\n",
    "     dropout1_p = .15,\n",
    "     hidden2_num_units = 64,\n",
    "     dropout2_p = .15,\n",
    "               \n",
    "     output_num_units = np.unique(y_train).shape[0],\n",
    "     output_nonlinearity = nonlinearities.softmax,\n",
    "               \n",
    "     update_learning_rate=theano.shared(float32(0.03)),\n",
    "     update_momentum=theano.shared(float32(0.9)),\n",
    "    \n",
    "     batch_iterator_train=BatchIterator(batch_size=1024),\n",
    "               \n",
    "     on_epoch_finished=[\n",
    "        AdjustVariable('update_learning_rate', start=0.03, stop=0.0001),\n",
    "        AdjustVariable('update_momentum', start=0.9, stop=0.999),\n",
    "        EarlyStopping(patience=25)\n",
    "        ],\n",
    "\n",
    "     regression = False,\n",
    "     max_epochs = 200,\n",
    "     verbose = True\n",
    "      )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Neural Network with 1300966 learnable parameters\n",
      "\n",
      "## Layer information\n",
      "\n",
      "  #  name        size\n",
      "---  --------  ------\n",
      "  0  input      10079\n",
      "  1  dropout    10079\n",
      "  2  hidden1      128\n",
      "  3  dropout1     128\n",
      "  4  hidden2       64\n",
      "  5  dropout2      64\n",
      "  6  output        38\n",
      "\n",
      "  epoch    train loss    valid loss    train/val    valid acc  dur\n",
      "-------  ------------  ------------  -----------  -----------  -----\n",
      "      1       \u001b[36m2.88408\u001b[0m       \u001b[32m2.16694\u001b[0m      1.33095      0.42873  2.64s\n",
      "      2       \u001b[36m2.10599\u001b[0m       \u001b[32m1.65165\u001b[0m      1.27508      0.58642  2.55s\n",
      "      3       \u001b[36m1.84701\u001b[0m       \u001b[32m1.44312\u001b[0m      1.27988      0.61387  2.40s\n",
      "      4       \u001b[36m1.70986\u001b[0m       \u001b[32m1.31878\u001b[0m      1.29655      0.63887  2.55s\n",
      "      5       \u001b[36m1.63761\u001b[0m       \u001b[32m1.25250\u001b[0m      1.30748      0.64903  2.52s\n",
      "      6       \u001b[36m1.56697\u001b[0m       \u001b[32m1.20155\u001b[0m      1.30412      0.65784  2.90s\n",
      "      7       \u001b[36m1.52231\u001b[0m       \u001b[32m1.15751\u001b[0m      1.31516      0.66172  2.72s\n",
      "      8       \u001b[36m1.48413\u001b[0m       \u001b[32m1.12447\u001b[0m      1.31984      0.66433  2.51s\n",
      "      9       \u001b[36m1.45901\u001b[0m       \u001b[32m1.09833\u001b[0m      1.32839      0.67132  2.33s\n",
      "     10       \u001b[36m1.42569\u001b[0m       \u001b[32m1.07211\u001b[0m      1.32980      0.67842  2.44s\n",
      "     11       \u001b[36m1.40457\u001b[0m       \u001b[32m1.04011\u001b[0m      1.35040      0.68614  2.31s\n",
      "     12       \u001b[36m1.37868\u001b[0m       \u001b[32m1.02662\u001b[0m      1.34293      0.68511  2.31s\n",
      "     13       \u001b[36m1.35751\u001b[0m       \u001b[32m1.00797\u001b[0m      1.34677      0.69273  2.31s\n",
      "     14       \u001b[36m1.34263\u001b[0m       \u001b[32m1.00148\u001b[0m      1.34064      0.69379  2.32s\n",
      "     15       \u001b[36m1.32894\u001b[0m       \u001b[32m0.99133\u001b[0m      1.34057      0.69540  2.36s\n",
      "     16       \u001b[36m1.30707\u001b[0m       \u001b[32m0.99124\u001b[0m      1.31862      0.68909  2.30s\n",
      "     17       \u001b[36m1.29171\u001b[0m       \u001b[32m0.97237\u001b[0m      1.32841      0.69285  2.22s\n",
      "     18       \u001b[36m1.28247\u001b[0m       \u001b[32m0.95622\u001b[0m      1.34119      0.69773  2.15s\n",
      "     19       \u001b[36m1.26374\u001b[0m       0.96279      1.31258      0.69439  2.30s\n",
      "     20       \u001b[36m1.25706\u001b[0m       \u001b[32m0.94664\u001b[0m      1.32792      0.69912  2.26s\n",
      "     21       \u001b[36m1.23757\u001b[0m       0.94981      1.30297      0.69597  2.21s\n",
      "     22       \u001b[36m1.23648\u001b[0m       \u001b[32m0.92446\u001b[0m      1.33751      0.70570  2.30s\n",
      "     23       \u001b[36m1.21964\u001b[0m       \u001b[32m0.91765\u001b[0m      1.32908      0.71119  2.73s\n",
      "     24       \u001b[36m1.20633\u001b[0m       \u001b[32m0.91234\u001b[0m      1.32224      0.70737  2.41s\n",
      "     25       \u001b[36m1.19670\u001b[0m       0.91267      1.31120      0.70906  2.30s\n",
      "     26       \u001b[36m1.18899\u001b[0m       \u001b[32m0.90843\u001b[0m      1.30884      0.70898  2.28s\n",
      "     27       \u001b[36m1.18233\u001b[0m       \u001b[32m0.88060\u001b[0m      1.34264      0.71911  2.29s\n",
      "     28       \u001b[36m1.16896\u001b[0m       \u001b[32m0.87967\u001b[0m      1.32886      0.72011  2.32s\n",
      "     29       \u001b[36m1.15661\u001b[0m       \u001b[32m0.87905\u001b[0m      1.31575      0.71981  2.31s\n",
      "     30       \u001b[36m1.14154\u001b[0m       \u001b[32m0.87485\u001b[0m      1.30484      0.71764  2.32s\n",
      "     31       1.14875       0.88719      1.29482      0.71046  2.31s\n",
      "     32       \u001b[36m1.13993\u001b[0m       0.88906      1.28217      0.71128  2.32s\n",
      "     33       \u001b[36m1.12081\u001b[0m       \u001b[32m0.87420\u001b[0m      1.28210      0.71737  2.28s\n",
      "     34       \u001b[36m1.11071\u001b[0m       \u001b[32m0.85497\u001b[0m      1.29912      0.72666  2.61s\n",
      "     35       \u001b[36m1.10563\u001b[0m       0.87089      1.26955      0.71603  2.32s\n",
      "     36       1.10679       \u001b[32m0.83703\u001b[0m      1.32229      0.73321  2.32s\n",
      "     37       \u001b[36m1.10158\u001b[0m       0.85687      1.28559      0.72757  2.30s\n",
      "     38       \u001b[36m1.08351\u001b[0m       0.84104      1.28829      0.72954  2.29s\n",
      "     39       \u001b[36m1.07686\u001b[0m       0.85013      1.26671      0.72485  2.29s\n",
      "     40       \u001b[36m1.06562\u001b[0m       0.84726      1.25772      0.72434  2.32s\n",
      "     41       \u001b[36m1.06469\u001b[0m       0.84652      1.25773      0.72342  2.32s\n",
      "     42       \u001b[36m1.06285\u001b[0m       \u001b[32m0.83578\u001b[0m      1.27169      0.72851  2.41s\n",
      "     43       \u001b[36m1.05557\u001b[0m       0.84593      1.24782      0.72379  2.29s\n",
      "     44       \u001b[36m1.04625\u001b[0m       0.87319      1.19819      0.71208  2.32s\n",
      "     45       1.04676       0.87236      1.19992      0.71915  2.24s\n",
      "     46       \u001b[36m1.03917\u001b[0m       \u001b[32m0.83102\u001b[0m      1.25049      0.72605  2.18s\n",
      "     47       \u001b[36m1.02614\u001b[0m       0.83723      1.22564      0.72765  2.31s\n",
      "     48       \u001b[36m1.01996\u001b[0m       0.86294      1.18195      0.71905  2.23s\n",
      "     49       \u001b[36m1.01636\u001b[0m       0.84805      1.19847      0.71994  2.32s\n",
      "     50       \u001b[36m1.01613\u001b[0m       0.84449      1.20325      0.72037  2.32s\n",
      "     51       \u001b[36m1.01122\u001b[0m       0.87364      1.15747      0.71002  2.31s\n",
      "     52       \u001b[36m1.00077\u001b[0m       0.85200      1.17462      0.72154  2.29s\n",
      "     53       1.00301       \u001b[32m0.81292\u001b[0m      1.23383      0.73289  2.32s\n",
      "     54       \u001b[36m0.99153\u001b[0m       \u001b[32m0.81285\u001b[0m      1.21981      0.73594  2.32s\n",
      "     55       \u001b[36m0.98474\u001b[0m       0.84441      1.16618      0.72338  2.15s\n",
      "     56       \u001b[36m0.98172\u001b[0m       \u001b[32m0.80874\u001b[0m      1.21389      0.73737  2.32s\n",
      "     57       0.98302       0.83447      1.17802      0.72160  2.32s\n",
      "     58       \u001b[36m0.97165\u001b[0m       0.82794      1.17357      0.73031  2.32s\n",
      "     59       \u001b[36m0.96410\u001b[0m       0.85056      1.13349      0.72203  2.31s\n",
      "     60       \u001b[36m0.95866\u001b[0m       0.82982      1.15526      0.72869  2.30s\n",
      "     61       0.96423       0.83912      1.14908      0.72632  2.32s\n",
      "     62       \u001b[36m0.95438\u001b[0m       0.82703      1.15398      0.72882  2.31s\n",
      "     63       0.95783       \u001b[32m0.80502\u001b[0m      1.18981      0.73573  2.32s\n",
      "     64       \u001b[36m0.94861\u001b[0m       0.81823      1.15935      0.73421  2.32s\n",
      "     65       \u001b[36m0.94194\u001b[0m       \u001b[32m0.78477\u001b[0m      1.20027      0.74372  2.31s\n",
      "     66       \u001b[36m0.94130\u001b[0m       0.80068      1.17562      0.73709  2.31s\n",
      "     67       \u001b[36m0.93103\u001b[0m       0.84398      1.10315      0.71897  2.32s\n",
      "     68       0.94262       0.85031      1.10856      0.72387  2.32s\n",
      "     69       \u001b[36m0.92816\u001b[0m       0.80997      1.14592      0.73822  2.32s\n",
      "     70       \u001b[36m0.92369\u001b[0m       0.84675      1.09087      0.73237  2.31s\n",
      "     71       0.92573       0.83850      1.10403      0.72536  2.28s\n",
      "     72       \u001b[36m0.92064\u001b[0m       0.82029      1.12234      0.73281  2.31s\n",
      "     73       \u001b[36m0.91910\u001b[0m       0.79670      1.15363      0.73547  2.37s\n",
      "     74       \u001b[36m0.91363\u001b[0m       0.80320      1.13749      0.73461  2.74s\n",
      "     75       \u001b[36m0.89909\u001b[0m       0.81761      1.09966      0.72833  2.50s\n",
      "     76       0.91352       0.81891      1.11553      0.73317  2.25s\n",
      "     77       0.91369       0.80245      1.13863      0.73957  2.27s\n",
      "     78       \u001b[36m0.89658\u001b[0m       0.80190      1.11807      0.73452  2.30s\n",
      "     79       0.90062       0.82318      1.09408      0.72485  2.31s\n",
      "     80       \u001b[36m0.88584\u001b[0m       \u001b[32m0.78216\u001b[0m      1.13255      0.74409  2.32s\n",
      "     81       0.89614       0.81404      1.10086      0.72811  2.32s\n",
      "     82       \u001b[36m0.88525\u001b[0m       0.81077      1.09186      0.74133  2.29s\n",
      "     83       0.89041       0.79524      1.11967      0.74173  2.31s\n",
      "     84       \u001b[36m0.88376\u001b[0m       0.80941      1.09187      0.73233  2.28s\n",
      "     85       \u001b[36m0.86855\u001b[0m       0.80893      1.07371      0.73184  2.30s\n",
      "     86       0.87865       0.81239      1.08156      0.73815  2.47s\n",
      "     87       0.86989       0.80088      1.08617      0.74352  2.71s\n",
      "     88       0.86970       0.80420      1.08145      0.73769  2.74s\n",
      "     89       0.86901       0.78639      1.10505      0.74503  2.37s\n",
      "     90       \u001b[36m0.86729\u001b[0m       0.81550      1.06352      0.72874  2.32s\n",
      "     91       \u001b[36m0.86163\u001b[0m       0.83588      1.03080      0.72696  2.31s\n",
      "     92       \u001b[36m0.85454\u001b[0m       0.78340      1.09081      0.74452  2.31s\n",
      "     93       0.85561       0.82815      1.03316      0.72775  2.31s\n",
      "     94       0.86279       0.78926      1.09317      0.74734  2.32s\n",
      "     95       0.85469       0.79960      1.06890      0.73402  2.32s\n",
      "     96       \u001b[36m0.84513\u001b[0m       0.78769      1.07292      0.73991  2.31s\n",
      "     97       \u001b[36m0.83966\u001b[0m       \u001b[32m0.77601\u001b[0m      1.08202      0.74879  2.38s\n",
      "     98       \u001b[36m0.82305\u001b[0m       0.80568      1.02156      0.73413  2.74s\n",
      "     99       0.84553       0.81514      1.03728      0.73154  2.33s\n",
      "    100       0.83718       0.81963      1.02142      0.72914  2.32s\n",
      "    101       0.83271       0.81062      1.02724      0.73160  2.31s\n",
      "    102       \u001b[36m0.82254\u001b[0m       0.79446      1.03535      0.74003  2.29s\n",
      "    103       \u001b[36m0.82011\u001b[0m       0.79708      1.02889      0.73887  2.30s\n",
      "    104       0.82389       0.83933      0.98160      0.72212  2.31s\n",
      "    105       \u001b[36m0.81792\u001b[0m       0.79819      1.02471      0.74377  2.31s\n",
      "    106       0.82216       0.81096      1.01381      0.73689  2.28s\n",
      "    107       0.83618       0.80529      1.03836      0.74005  2.30s\n",
      "    108       0.82453       0.80655      1.02230      0.73563  2.31s\n",
      "    109       0.82569       0.81492      1.01322      0.72780  2.31s\n",
      "    110       0.82377       0.79148      1.04079      0.74331  2.30s\n",
      "    111       0.82514       0.94272      0.87527      0.71804  2.51s\n",
      "    112       0.86526       0.80125      1.07988      0.73553  2.41s\n",
      "    113       0.82891       0.80148      1.03422      0.73927  2.32s\n",
      "    114       0.82491       0.79092      1.04297      0.74313  2.30s\n",
      "    115       0.83534       0.81253      1.02807      0.73439  2.31s\n",
      "    116       0.82235       0.82010      1.00274      0.73244  2.29s\n",
      "    117       0.82580       0.81224      1.01669      0.73612  2.31s\n",
      "    118       0.82232       0.79235      1.03783      0.74204  2.31s\n",
      "    119       \u001b[36m0.81719\u001b[0m       0.81178      1.00666      0.73896  2.30s\n",
      "    120       \u001b[36m0.81390\u001b[0m       0.78876      1.03188      0.74147  2.24s\n",
      "    121       \u001b[36m0.79880\u001b[0m       0.79578      1.00380      0.74391  2.30s\n",
      "    122       0.80547       0.80687      0.99826      0.73798  2.31s\n",
      "    123       0.80010       0.80675      0.99175      0.73465  2.32s\n",
      "    124       \u001b[36m0.79639\u001b[0m       0.84352      0.94413      0.72562  2.31s\n",
      "    125       0.80732       0.84015      0.96092      0.73253  2.31s\n",
      "    126       0.80423       0.78446      1.02520      0.74641  2.31s\n",
      "    127       \u001b[36m0.79359\u001b[0m       0.82365      0.96351      0.73057  2.32s\n",
      "    128       \u001b[36m0.78100\u001b[0m       0.79027      0.98827      0.73960  2.29s\n",
      "    129       0.78802       0.79947      0.98567      0.73894  2.31s\n",
      "    130       0.79222       0.80915      0.97908      0.73066  2.32s\n",
      "    131       0.79529       0.81419      0.97678      0.73389  2.31s\n",
      "    132       0.79511       0.79562      0.99936      0.73790  2.31s\n",
      "    133       0.79119       0.80073      0.98808      0.73768  2.30s\n",
      "    134       \u001b[36m0.78079\u001b[0m       0.82137      0.95059      0.73556  2.31s\n",
      "    135       \u001b[36m0.77745\u001b[0m       0.80987      0.95997      0.73504  2.31s\n",
      "    136       \u001b[36m0.77128\u001b[0m       0.79824      0.96623      0.74384  2.30s\n",
      "    137       \u001b[36m0.77082\u001b[0m       0.82492      0.93442      0.72972  2.31s\n",
      "    138       0.77509       0.79388      0.97634      0.74498  2.32s\n",
      "    139       \u001b[36m0.76558\u001b[0m       0.79768      0.95975      0.74360  2.32s\n",
      "    140       0.77023       0.82172      0.93733      0.73612  2.31s\n",
      "    141       0.76858       0.80751      0.95180      0.73866  2.31s\n",
      "    142       \u001b[36m0.75113\u001b[0m       0.79115      0.94942      0.74246  2.31s\n",
      "    143       0.76086       0.80416      0.94616      0.73718  2.31s\n",
      "    144       \u001b[36m0.74400\u001b[0m       0.81927      0.90812      0.73237  2.31s\n",
      "    145       0.75403       0.79476      0.94875      0.74402  2.31s\n",
      "    146       0.75383       0.79874      0.94377      0.74131  2.32s\n",
      "    147       0.74880       0.79112      0.94651      0.73967  2.16s\n",
      "    148       0.75609       0.81097      0.93233      0.74074  2.25s\n",
      "    149       0.75695       0.81538      0.92834      0.73888  2.31s\n",
      "    150       0.74866       0.81584      0.91766      0.73524  2.29s\n",
      "    151       \u001b[36m0.74054\u001b[0m       0.83374      0.88821      0.73883  2.28s\n",
      "    152       0.74327       0.82282      0.90332      0.73315  2.31s\n",
      "    153       0.74940       0.79766      0.93949      0.73949  2.31s\n",
      "    154       \u001b[36m0.74009\u001b[0m       0.79410      0.93198      0.74354  2.31s\n",
      "    155       0.74604       0.82693      0.90218      0.73528  2.30s\n",
      "    156       \u001b[36m0.73848\u001b[0m       0.80498      0.91738      0.73930  2.30s\n",
      "    157       \u001b[36m0.72966\u001b[0m       0.82735      0.88193      0.73641  2.32s\n",
      "    158       0.73478       0.80765      0.90977      0.74107  2.30s\n",
      "    159       \u001b[36m0.72370\u001b[0m       0.81873      0.88393      0.73670  2.29s\n",
      "    160       0.73542       0.80300      0.91584      0.74065  2.31s\n",
      "    161       0.72730       0.84585      0.85984      0.72747  2.30s\n",
      "    162       0.73137       0.81248      0.90017      0.74207  2.31s\n",
      "    163       0.72614       0.82401      0.88123      0.73028  2.29s\n",
      "    164       \u001b[36m0.71700\u001b[0m       0.83136      0.86245      0.73238  2.30s\n",
      "    165       0.71760       0.80520      0.89120      0.73990  2.31s\n",
      "    166       0.72416       0.80251      0.90237      0.73997  2.31s\n",
      "    167       0.72387       0.82656      0.87577      0.73556  2.30s\n",
      "    168       0.72370       0.81402      0.88904      0.73906  2.30s\n",
      "    169       0.73058       0.82371      0.88694      0.73736  2.32s\n",
      "    170       0.72372       0.81541      0.88755      0.73994  2.31s\n",
      "    171       0.72830       0.82762      0.87999      0.73274  2.31s\n",
      "    172       \u001b[36m0.71236\u001b[0m       0.80078      0.88958      0.74602  2.32s\n",
      "    173       0.71856       0.82397      0.87208      0.73719  2.31s\n",
      "    174       \u001b[36m0.71165\u001b[0m       0.84278      0.84441      0.72972  2.31s\n",
      "    175       0.72337       0.82872      0.87287      0.73194  2.23s\n",
      "    176       0.71182       0.82389      0.86398      0.73708  2.30s\n",
      "    177       0.71789       0.82077      0.87466      0.74010  2.32s\n",
      "    178       \u001b[36m0.70795\u001b[0m       0.81817      0.86528      0.74274  2.31s\n",
      "    179       0.71351       0.81931      0.87087      0.73908  2.31s\n",
      "    180       \u001b[36m0.70757\u001b[0m       0.80559      0.87832      0.74307  2.30s\n",
      "    181       \u001b[36m0.70654\u001b[0m       0.81806      0.86367      0.73894  2.29s\n",
      "    182       \u001b[36m0.70354\u001b[0m       0.84030      0.83724      0.73105  2.31s\n",
      "    183       0.70651       0.82585      0.85550      0.73787  2.31s\n",
      "    184       \u001b[36m0.69575\u001b[0m       0.84264      0.82568      0.73347  2.27s\n",
      "    185       0.70522       0.82169      0.85826      0.73546  2.30s\n",
      "    186       0.70409       0.85202      0.82637      0.73321  2.31s\n",
      "    187       0.69861       0.86861      0.80429      0.72931  2.30s\n",
      "    188       \u001b[36m0.69290\u001b[0m       0.82176      0.84319      0.73929  2.31s\n",
      "    189       0.69867       0.82126      0.85072      0.73961  2.30s\n",
      "    190       0.69601       0.81423      0.85480      0.73983  2.31s\n",
      "    191       \u001b[36m0.69037\u001b[0m       0.82512      0.83670      0.74330  2.31s\n",
      "    192       0.69443       0.87400      0.79455      0.72686  2.31s\n",
      "    193       0.69552       0.86168      0.80716      0.72884  2.31s\n",
      "    194       \u001b[36m0.68560\u001b[0m       0.82891      0.82711      0.74073  2.31s\n",
      "    195       0.69230       0.85451      0.81017      0.73381  2.31s\n",
      "    196       0.68945       0.84300      0.81785      0.73345  2.30s\n",
      "    197       0.69912       0.85132      0.82122      0.73061  2.31s\n",
      "    198       0.68902       0.82824      0.83191      0.73629  2.31s\n",
      "    199       0.69202       0.86133      0.80343      0.73094  2.29s\n",
      "    200       \u001b[36m0.68315\u001b[0m       0.83678      0.81641      0.73680  2.30s\n",
      "    201       \u001b[36m0.68290\u001b[0m       0.84832      0.80501      0.73087  2.31s\n",
      "    202       \u001b[36m0.68198\u001b[0m       0.84544      0.80666      0.73217  2.32s\n",
      "    203       \u001b[36m0.67304\u001b[0m       0.85123      0.79067      0.73083  2.30s\n",
      "    204       0.68031       0.83118      0.81849      0.73850  2.30s\n",
      "    205       0.67716       0.84225      0.80399      0.73480  2.32s\n",
      "    206       0.67608       0.88860      0.76084      0.72257  2.31s\n",
      "    207       0.67868       0.85246      0.79614      0.73013  2.28s\n",
      "    208       \u001b[36m0.67105\u001b[0m       0.85592      0.78401      0.73002  2.31s\n",
      "    209       0.67538       0.83875      0.80523      0.73373  2.31s\n",
      "    210       0.67677       0.85539      0.79118      0.73399  2.31s\n",
      "    211       \u001b[36m0.66993\u001b[0m       0.84479      0.79301      0.73391  2.28s\n",
      "    212       \u001b[36m0.66425\u001b[0m       0.84645      0.78475      0.73417  2.32s\n",
      "    213       \u001b[36m0.65881\u001b[0m       0.83062      0.79316      0.73892  2.30s\n",
      "    214       0.66529       0.83749      0.79438      0.73826  2.32s\n",
      "    215       0.66316       0.83054      0.79847      0.73458  2.31s\n",
      "    216       0.66459       0.86164      0.77131      0.72991  2.31s\n",
      "    217       0.66514       0.85901      0.77430      0.73036  2.30s\n",
      "    218       0.65919       0.83754      0.78706      0.73706  2.31s\n",
      "    219       0.66253       0.83858      0.79006      0.73641  2.31s\n",
      "    220       0.65902       0.83442      0.78979      0.73922  2.32s\n",
      "    221       0.66377       0.84915      0.78168      0.73197  2.32s\n",
      "    222       \u001b[36m0.65319\u001b[0m       0.83488      0.78237      0.73567  2.31s\n",
      "    223       0.65867       0.88633      0.74314      0.72571  2.32s\n",
      "    224       0.66417       0.85600      0.77590      0.73432  2.32s\n",
      "    225       0.65918       0.88052      0.74863      0.72802  2.32s\n",
      "    226       0.65657       0.87692      0.74872      0.72952  2.32s\n",
      "    227       \u001b[36m0.64969\u001b[0m       0.86713      0.74924      0.73137  2.31s\n",
      "    228       0.65518       0.84919      0.77154      0.73423  2.27s\n",
      "    229       0.65578       0.88663      0.73964      0.72252  2.30s\n",
      "    230       0.66151       0.85311      0.77542      0.73277  2.32s\n",
      "    231       0.65196       0.86232      0.75605      0.72991  2.31s\n",
      "    232       0.65517       0.83950      0.78042      0.73644  2.27s\n",
      "    233       0.65566       0.84796      0.77322      0.73392  2.19s\n",
      "    234       0.65515       0.87287      0.75057      0.73106  2.26s\n",
      "    235       0.65658       0.85271      0.77000      0.73240  2.27s\n",
      "    236       0.65547       0.84337      0.77720      0.73671  2.31s\n",
      "    237       0.65336       0.87977      0.74265      0.73308  2.31s\n",
      "    238       \u001b[36m0.64963\u001b[0m       0.83391      0.77902      0.74349  2.31s\n",
      "    239       \u001b[36m0.64817\u001b[0m       0.87330      0.74221      0.73181  2.27s\n",
      "    240       \u001b[36m0.64410\u001b[0m       0.86305      0.74631      0.72892  2.32s\n",
      "    241       0.64889       0.85632      0.75777      0.73148  2.31s\n",
      "    242       0.65349       0.85388      0.76532      0.73528  2.24s\n",
      "    243       0.64597       0.86784      0.74435      0.73416  2.30s\n",
      "    244       \u001b[36m0.64289\u001b[0m       0.85583      0.75119      0.73513  2.31s\n",
      "    245       0.65042       0.85922      0.75699      0.72847  2.30s\n",
      "    246       \u001b[36m0.64242\u001b[0m       0.86325      0.74419      0.73258  2.31s\n",
      "    247       0.64860       0.86354      0.75109      0.73458  2.32s\n",
      "    248       \u001b[36m0.63547\u001b[0m       0.83800      0.75831      0.73856  2.32s\n",
      "    249       0.63913       0.85099      0.75104      0.73647  2.31s\n",
      "    250       \u001b[36m0.63441\u001b[0m       0.84358      0.75205      0.73984  2.31s\n",
      "    251       0.64256       0.85229      0.75393      0.73668  2.31s\n",
      "    252       \u001b[36m0.63108\u001b[0m       0.85798      0.73555      0.73329  2.32s\n",
      "    253       0.63369       0.86702      0.73087      0.73309  2.32s\n",
      "    254       0.63305       0.88930      0.71185      0.72601  2.31s\n",
      "    255       0.63818       0.86790      0.73531      0.73655  2.31s\n",
      "    256       0.65109       0.86130      0.75594      0.73385  2.31s\n",
      "    257       \u001b[36m0.62344\u001b[0m       0.86529      0.72050      0.73777  2.32s\n",
      "    258       0.63320       0.85776      0.73819      0.73548  2.31s\n",
      "    259       0.63920       0.88407      0.72301      0.73237  2.29s\n",
      "    260       0.63214       0.86730      0.72886      0.73121  2.30s\n",
      "    261       0.62943       0.86691      0.72607      0.73552  2.31s\n",
      "    262       0.63421       0.87959      0.72103      0.73082  2.32s\n",
      "    263       0.62495       0.88156      0.70892      0.73197  2.32s\n",
      "    264       0.63537       0.88606      0.71707      0.72678  2.32s\n",
      "    265       0.63727       0.88098      0.72337      0.73096  2.31s\n",
      "    266       0.62584       0.88877      0.70416      0.72497  2.32s\n",
      "    267       0.62938       0.90617      0.69455      0.72415  2.32s\n",
      "    268       0.64535       0.87543      0.73718      0.73022  2.31s\n",
      "    269       0.63304       0.87537      0.72317      0.72770  2.31s\n",
      "    270       0.63703       0.91760      0.69424      0.72375  2.31s\n",
      "    271       0.62679       0.88194      0.71069      0.72991  2.31s\n",
      "    272       0.63206       0.90255      0.70030      0.72545  2.32s\n",
      "    273       0.63462       0.91046      0.69703      0.72422  2.32s\n",
      "    274       0.63150       0.89452      0.70596      0.72746  2.31s\n",
      "    275       0.62451       0.90426      0.69063      0.72336  2.31s\n",
      "    276       0.62750       0.89601      0.70033      0.73079  2.32s\n",
      "    277       0.63249       0.91447      0.69165      0.72629  2.32s\n",
      "    278       0.64131       0.89669      0.71519      0.72587  2.30s\n",
      "    279       0.62932       0.88984      0.70723      0.72483  2.30s\n",
      "    280       0.62856       0.88240      0.71233      0.72672  2.31s\n",
      "    281       0.62740       0.89659      0.69976      0.72443  2.30s\n",
      "    282       0.63240       0.90966      0.69521      0.72425  2.31s\n",
      "    283       0.62567       0.90721      0.68967      0.72807  2.29s\n",
      "    284       0.62539       0.90576      0.69046      0.72367  2.31s\n",
      "    285       0.63038       0.89586      0.70366      0.72675  2.29s\n",
      "    286       0.63153       0.88914      0.71027      0.72739  2.32s\n",
      "    287       0.62427       0.89310      0.69899      0.72706  2.27s\n",
      "    288       \u001b[36m0.61770\u001b[0m       0.90259      0.68436      0.72827  2.31s\n",
      "    289       0.62614       0.89637      0.69852      0.72178  2.31s\n",
      "    290       0.62844       0.91122      0.68967      0.72069  2.29s\n",
      "    291       0.63875       0.89453      0.71406      0.72385  2.31s\n",
      "    292       0.62483       0.89804      0.69577      0.72183  2.31s\n",
      "    293       0.62330       0.90616      0.68784      0.72257  2.31s\n",
      "    294       0.64109       0.90599      0.70761      0.72198  2.31s\n",
      "    295       0.63312       0.90013      0.70336      0.72740  2.31s\n",
      "    296       0.63056       0.89888      0.70149      0.72841  2.31s\n",
      "    297       0.62438       0.86685      0.72028      0.73695  2.31s\n",
      "    298       0.62599       0.86899      0.72036      0.73459  2.28s\n",
      "    299       0.63063       0.88899      0.70938      0.72429  2.31s\n",
      "    300       0.64013       0.91604      0.69881      0.72439  2.32s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NeuralNet(X_tensor_type=None,\n",
       "     batch_iterator_test=<nolearn.lasagne.base.BatchIterator object at 0x7fdae13ed310>,\n",
       "     batch_iterator_train=<nolearn.lasagne.base.BatchIterator object at 0x7fdad5ddf410>,\n",
       "     custom_score=None, dropout1_p=0.2, dropout2_p=0.2, dropout_p=0.25,\n",
       "     hidden1_num_units=128, hidden2_num_units=64,\n",
       "     input_shape=(None, 10079),\n",
       "     layers=[('input', <class 'lasagne.layers.input.InputLayer'>), ('dropout', <class 'lasagne.layers.noise.DropoutLayer'>), ('hidden1', <class 'lasagne.layers.dense.DenseLayer'>), ('dropout1', <class 'lasagne.layers.noise.DropoutLayer'>), ('hidden2', <class 'lasagne.layers.dense.DenseLayer'>), ('dropout2', <class 'lasagne.layers.noise.DropoutLayer'>), ('output', <class 'lasagne.layers.dense.DenseLayer'>)],\n",
       "     loss=None, max_epochs=300, more_params={},\n",
       "     objective=<function objective at 0x7fdae13d7e60>,\n",
       "     objective_loss_function=<function categorical_crossentropy at 0x7fdae142caa0>,\n",
       "     on_epoch_finished=[<__main__.AdjustVariable object at 0x7fdad5ddf6d0>, <__main__.AdjustVariable object at 0x7fdad5ddf710>, <nolearn.lasagne.handlers.PrintLog instance at 0x7fdad5146128>],\n",
       "     on_training_finished=[],\n",
       "     on_training_started=[<nolearn.lasagne.handlers.PrintLayerInfo instance at 0x7fdad5146a28>],\n",
       "     output_nonlinearity=<function softmax at 0x7fdae1870a28>,\n",
       "     output_num_units=38, regression=False,\n",
       "     train_split=<nolearn.lasagne.base.TrainSplit object at 0x7fdae13ed350>,\n",
       "     update=<function nesterov_momentum at 0x7fdae14382a8>,\n",
       "     update_learning_rate=<CudaNdarrayType(float32, scalar)>,\n",
       "     update_momentum=<CudaNdarrayType(float32, scalar)>,\n",
       "     use_label_encoder=False, verbose=True,\n",
       "     y_tensor_type=TensorType(int32, vector))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Will train until validation_0 error hasn't decreased in 25 rounds.\n",
      "[0]\tvalidation_0-mlogloss:3.167847\n",
      "[1]\tvalidation_0-mlogloss:2.830302\n",
      "[2]\tvalidation_0-mlogloss:2.613766\n",
      "[3]\tvalidation_0-mlogloss:2.477340\n",
      "[4]\tvalidation_0-mlogloss:2.369453\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, colsample_bytree=0.5, gamma=0,\n",
       "       learning_rate=0.1, max_delta_step=0, max_depth=14,\n",
       "       min_child_weight=1, missing=None, n_estimators=5, nthread=-1,\n",
       "       objective='multi:softprob', seed=0, silent=True, subsample=0.8)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_, X_val, y_, y_val = train_test_split(X_train, y_train, test_size = 25000, random_state = 13)\n",
    "\n",
    "del X_\n",
    "del y_\n",
    "\n",
    "xgb = xgboost.XGBClassifier(max_depth = 14, n_estimators = 200,\n",
    "                        objective='multi:softprob', subsample = .80, colsample_bytree=.5, )\n",
    "\n",
    "xgb.fit(X_train, y_train, eval_set = [(X_val, y_val)], eval_metric = 'mlogloss', early_stopping_rounds=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_xgb_train_predictions = xgb.predict_proba(X_train)\n",
    "y_nn_train_predictions = nn.predict_proba(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_ensembl_train = np.concatenate((y_xgb_train_predictions, y_nn_train_predictions), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "log_ensembl = LogisticRegression(C=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, penalty='l2', random_state=None, tol=0.0001)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_ensembl.fit(X_ensembl_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9084412290182412"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_ensembl.score(X_ensembl_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_xgb_test_predictions = xgb.predict_proba(X_test)\n",
    "y_nn_test_predictions = nn.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_ensembl_test = np.concatenate((y_xgb_test_predictions, y_nn_test_predictions), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_probas = log_ensembl.predict_proba(X_ensembl_test)\n",
    "\n",
    "\n",
    "col_names = ['TripType_' + str(c) for c in enc.classes_.astype('int')]\n",
    "submission = pd.DataFrame(np.round(y_probas, 4), index=y_df[pd.isnull(y_df.TripType)].index, columns = col_names)\n",
    "\n",
    "submission.reset_index(inplace = True)\n",
    "submission.to_csv('Walmart_log_ensembl_10000Features-Notebook.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_probas_avg = (y_xgb_test_predictions + y_nn_test_preditions)/2\n",
    "submission = pd.DataFrame(np.round(y_probas_avg,4), index=y_df[pd.isnull(y_df.TripType)].index, columns = col_names)\n",
    "\n",
    "submission.reset_index(inplace = True)\n",
    "submission.to_csv('Walmart_avg_ensembl_10000Features-Notebook.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
